diff --git a/Documentation/admin-guide/sysctl/kernel.rst b/Documentation/admin-guide/sysctl/kernel.rst
index f3ee807b5..8be285ad9 100644
--- a/Documentation/admin-guide/sysctl/kernel.rst
+++ b/Documentation/admin-guide/sysctl/kernel.rst
@@ -1229,6 +1229,12 @@ ROM/Flash boot loader. Maybe to tell it what to do after
 rebooting. ???
 
 
+sched_interactivity_factor (CacULE scheduler only)
+==================================================
+Sets the value *m* for interactivity score calculations. See
+Figure 1 in https://web.cs.ucdavis.edu/~roper/ecs150/ULE.pdf
+
+
 sched_energy_aware
 ==================
 
diff --git a/Documentation/scheduler/sched-CacULE.rst b/Documentation/scheduler/sched-CacULE.rst
new file mode 100644
index 000000000..82b0847c4
--- /dev/null
+++ b/Documentation/scheduler/sched-CacULE.rst
@@ -0,0 +1,76 @@
+======================================
+The CacULE Scheduler by Hamad Al Marri.
+======================================
+
+1.  Overview
+=============
+
+The CacULE CPU scheduler is based on interactivity score mechanism.
+The interactivity score is inspired by the ULE scheduler (FreeBSD
+scheduler).
+
+1.1 About CacULE Scheduler
+--------------------------
+
+  - Each CPU has its own runqueue.
+
+  - NORMAL runqueue is a linked list of sched_entities (instead of RB-Tree).
+
+  - RT and other runqueues are just the same as the CFS's.
+
+  - Wake up tasks preempt currently running tasks if its interactivity score value
+    is higher.
+
+
+1.2. Complexity
+----------------
+
+The complexity of Enqueue and Dequeue a task is O(1).
+
+The complexity of pick the next task is in O(n), where n is the number of tasks
+in a runqueue (each CPU has its own runqueue).
+
+Note: O(n) sounds scary, but usually for a machine with 4 CPUS where it is used
+for desktop or mobile jobs, the maximum number of runnable tasks might not
+exceeds 10 (at the pick next run time) - the idle tasks are excluded since they
+are dequeued when sleeping and enqueued when they wake up.
+
+
+2. The CacULE Interactivity Score
+=======================================================
+
+The interactivity score is inspired by the ULE scheduler (FreeBSD scheduler).
+For more information see: https://web.cs.ucdavis.edu/~roper/ecs150/ULE.pdf
+CacULE doesn't replace CFS with ULE, it only changes the CFS' pick next task
+mechanism to ULE's interactivity score mechanism for picking next task to run.
+
+
+2.3 sched_interactivity_factor
+=================
+Sets the value *m* for interactivity score calculations. See Figure 1 in
+https://web.cs.ucdavis.edu/~roper/ecs150/ULE.pdf
+The default value of in CacULE is 10 which means that the Maximum Interactive
+Score is 20 (since m = Maximum Interactive Score / 2).
+You can tune sched_interactivity_factor with sysctl command:
+
+	sysctl kernel.sched_interactivity_factor=50
+
+This command changes the sched_interactivity_factor from 10 to 50.
+
+
+3. Scheduling policies
+=======================
+
+CacULE some CFS, implements three scheduling policies:
+
+  - SCHED_NORMAL (traditionally called SCHED_OTHER): The scheduling
+    policy that is used for regular tasks.
+
+  - SCHED_BATCH: Does not preempt nearly as often as regular tasks
+    would, thereby allowing tasks to run longer and make better use of
+    caches but at the cost of interactivity. This is well suited for
+    batch jobs.
+
+  - SCHED_IDLE: This is even weaker than nice 19, but its not a true
+    idle timer scheduler in order to avoid to get into priority
+    inversion problems which would deadlock the machine.
diff --git a/include/linux/sched.h b/include/linux/sched.h
index b469878de..9caccc882 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -567,10 +567,24 @@ struct sched_statistics {
 #endif /* CONFIG_SCHEDSTATS */
 } ____cacheline_aligned;
 
+
+#ifdef CONFIG_CACULE_SCHED
+struct cacule_node {
+	struct cacule_node*		next;
+	struct cacule_node*		prev;
+	u64				cacule_start_time;
+	u64				last_run;
+	u64				vruntime;
+};
+#endif
+
 struct sched_entity {
 	/* For load-balancing: */
 	struct load_weight		load;
 	struct rb_node			run_node;
+#ifdef CONFIG_CACULE_SCHED
+	struct cacule_node		cacule_node;
+#endif
 	u64				deadline;
 	u64				min_vruntime;
 	u64				min_slice;
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 5a64582b0..7ee6f8b2c 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -12,6 +12,17 @@ extern unsigned long sysctl_hung_task_timeout_secs;
 enum { sysctl_hung_task_timeout_secs = 0 };
 #endif
 
+
+#ifdef CONFIG_CACULE_SCHED
+extern unsigned int interactivity_factor;
+extern unsigned int cacule_max_lifetime;
+extern unsigned int cache_factor;
+extern unsigned int cache_divisor;
+extern unsigned int starve_factor;
+extern unsigned int starve_divisor;
+extern int cacule_yield;
+#endif
+
 enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_NONE,
 	SCHED_TUNABLESCALING_LOG,
diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index bbcfdf12a..eea536adb 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -67,7 +67,6 @@ struct sched_domain_shared {
 	atomic_t	ref;
 	atomic_t	nr_busy_cpus;
 	int		has_idle_cores;
-	int		nr_idle_scan;
 };
 
 struct sched_domain {
diff --git a/init/Kconfig b/init/Kconfig
index cab3ad28c..71a1a8b41 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -911,6 +911,52 @@ config SCHED_PROXY_EXEC
 	  This option enables proxy execution, a mechanism for mutex-owning
 	  tasks to inherit the scheduling context of higher priority waiters.
 
+
+config CACULE_SCHED
+	bool "CacULE CPU scheduler"
+	default y
+	help
+	  The CacULE CPU scheduler is based on interactivity score mechanism.
+	  The interactivity score is inspired by the ULE scheduler (FreeBSD
+	  scheduler).
+
+	  If unsure, say Y here.
+
+config CACULE_RDB
+	bool "RDB (Response Driven Balancer)"
+	default y
+	depends on CACULE_SCHED
+	help
+	  This is an experimental load balancer for CacULE. It is a lightweight
+	  load balancer which is a replacement of CFS load balancer. It migrates
+	  tasks based on their interactivity scores.
+
+	  If unsure, say Y here.
+
+config RDB_INTERVAL
+	int "RDB load balancer interval"
+	default 19
+	depends on CACULE_RDB
+	help
+	  This is an interval to control load balance time period.
+	  The trigger_load_balance runs in every tick. For High HZ values, the
+	  load balance could be overwhelming. RDB load balance includes rq locking
+	  which can reduce the performance. The balance interval can help to avoid
+	  running load balance on every tick. For example, RDB_INTERVAL=3 will
+	  only run load balance every 3ms. Setting RDB_INTERVAL depends on HZ.
+	  If you want load balancer run every 2ms while HZ=500 then it is not
+	  needed and better to set RDB_INTERVAL=0 since 500HZ already (1000ms
+	  / 500HZ = 2ms). However, if you have 1000HZ and want to avoid load
+	  balancer from running every 1ms, you could set RDB_INTERVAL=4ms for
+	  example to make load balancer run every 4ms. Less RDB_INTERVAL values
+	  (or 0 to disable) could make sure tasks are balanced ASAP, but with
+	  the cost of locking/blocking time. High RDB_INTERVAL values can relax
+	  balancing locking but with the cost of imbalanced workload for that
+	  period of time (i.e. if RDB_INTERVAL=100ms) there will be no balancing
+	  for 100ms (except for newidle_balance which is not effected by RDB_INTERVAL).
+
+	  If in doubt, use the default value.
+
 endmenu
 
 #
@@ -1428,6 +1474,7 @@ config SCHED_AUTOGROUP
 	select CGROUPS
 	select CGROUP_SCHED
 	select FAIR_GROUP_SCHED
+	default y
 	help
 	  This option optimizes the scheduler for common desktop workloads by
 	  automatically creating and populating task groups.  This separation
diff --git a/kernel/Kconfig.hz b/kernel/Kconfig.hz
index ce1435cb0..c00543da7 100644
--- a/kernel/Kconfig.hz
+++ b/kernel/Kconfig.hz
@@ -46,6 +46,9 @@ choice
 	 1000 Hz is the preferred choice for desktop systems and other
 	 systems requiring fast interactive responses to events.
 
+	config HZ_2000
+		bool "2000 HZ"
+
 endchoice
 
 config HZ
@@ -54,6 +57,7 @@ config HZ
 	default 250 if HZ_250
 	default 300 if HZ_300
 	default 1000 if HZ_1000
+	default 2000 if HZ_2000
 
 config SCHED_HRTICK
 	def_bool HIGH_RES_TIMERS
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f754a60de..28eea3f19 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4447,6 +4447,11 @@ static void __sched_fork(u64 clone_flags, struct task_struct *p)
 	p->se.prev_sum_exec_runtime	= 0;
 	p->se.nr_migrations		= 0;
 	p->se.vruntime			= 0;
+
+#ifdef CONFIG_CACULE_SCHED
+	p->se.cacule_node.vruntime	= 0;
+#endif
+
 	p->se.vlag			= 0;
 	INIT_LIST_HEAD(&p->se.group_node);
 
@@ -4833,6 +4838,11 @@ void wake_up_new_task(struct task_struct *p)
 	update_rq_clock(rq);
 	post_init_entity_util_avg(p);
 
+#ifdef CONFIG_CACULE_SCHED
+	p->se.cacule_node.cacule_start_time = sched_clock();
+#endif
+
+
 	activate_task(rq, p, ENQUEUE_NOCLOCK | ENQUEUE_INITIAL);
 	trace_sched_wakeup_new(p);
 	wakeup_preempt(rq, p, wake_flags);
@@ -5686,6 +5696,7 @@ static void sched_tick_remote(struct work_struct *work)
 			WARN_ON_ONCE(rq->curr != rq->donor);
 			update_rq_clock(rq);
 
+#if !defined(CONFIG_CACULE_SCHED)
 			if (!is_idle_task(curr)) {
 				/*
 				 * Make sure the next tick runs within a
@@ -5694,6 +5705,8 @@ static void sched_tick_remote(struct work_struct *work)
 				u64 delta = rq_clock_task(rq) - curr->se.exec_start;
 				WARN_ON_ONCE(delta > (u64)NSEC_PER_SEC * 3);
 			}
+#endif
+
 			curr->sched_class->task_tick(rq, curr, 0);
 
 			calc_load_nohz_remote(rq);
@@ -8655,6 +8668,14 @@ void __init sched_init(void)
 	BUG_ON(!sched_class_above(&ext_sched_class, &idle_sched_class));
 #endif
 
+#ifdef CONFIG_CACULE_SCHED
+#ifdef CONFIG_CACULE_RDB
+	printk(KERN_INFO "CacULE CPU scheduler (RDB) v6.0 by Hamad Al Marri.");
+#else
+	printk(KERN_INFO "CacULE CPU scheduler v6.0 by Hamad Al Marri.");
+#endif
+#endif
+
 	wait_bit_init();
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 02e16b70a..2278a8a6b 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -819,15 +819,19 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	last = __pick_last_entity(cfs_rq);
 	if (last)
 		right_vruntime = last->vruntime;
+#if !defined(CONFIG_CACULE_SCHED)
 	min_vruntime = cfs_rq->min_vruntime;
 	raw_spin_rq_unlock_irqrestore(rq, flags);
+#endif
 
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "left_deadline",
 			SPLIT_NS(left_deadline));
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "left_vruntime",
 			SPLIT_NS(left_vruntime));
+#if !defined(CONFIG_CACULE_SCHED)
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "min_vruntime",
 			SPLIT_NS(min_vruntime));
+#endif
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "avg_vruntime",
 			SPLIT_NS(avg_vruntime(cfs_rq)));
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "right_vruntime",
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 71d3f4125..e9e71621c 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -19,6 +19,10 @@
  *
  *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra
  *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
+ *
+ *  CacULE enhancements CPU cache and scheduler based on
+ *  Interactivity Score.
+ *  (C) 2020 Hamad Al Marri <hamad.s.almarri@gmail.com>
  */
 #include <linux/energy_model.h>
 #include <linux/mmap_lock.h>
@@ -58,6 +62,22 @@
 #include "stats.h"
 #include "autogroup.h"
 
+#ifdef CONFIG_CACULE_SCHED
+unsigned int __read_mostly cacule_max_lifetime		= 11000; // in ms
+unsigned int __read_mostly interactivity_factor		= 65536;
+int __read_mostly cacule_yield				= 1;
+
+#define YIELD_MARK					0x8000000000000000ULL
+#define YIELD_UNMARK					0x7FFFFFFFFFFFFFFFULL
+
+unsigned int __read_mostly cache_factor			= 13107;
+unsigned int __read_mostly cache_divisor		= 1000000; // 1ms
+
+unsigned int __read_mostly starve_factor		= 19660;
+unsigned int __read_mostly starve_divisor		= 3000000; // 3ms
+#endif
+
+
 /*
  * The initial- and re-scaling of tunables is configurable
  *
@@ -152,6 +172,59 @@ static const struct ctl_table sched_fair_sysctls[] = {
 		.extra1		= SYSCTL_ZERO,
 	},
 #endif /* CONFIG_NUMA_BALANCING */
+#ifdef CONFIG_CACULE_SCHED
+	{
+		.procname	= "sched_interactivity_factor",
+		.data		= &interactivity_factor,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_max_lifetime_ms",
+		.data		= &cacule_max_lifetime,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_cache_factor",
+		.data		= &cache_factor,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_cache_divisor",
+		.data		= &cache_divisor,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_starve_factor",
+		.data		= &starve_factor,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_starve_divisor",
+		.data		= &starve_divisor,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "sched_cacule_yield",
+		.data		= &cacule_yield,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+#endif /* CONFIG_CACULE_SCHED */
 };
 
 static int __init sched_fair_sysctl_init(void)
@@ -297,6 +370,70 @@ static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)
 
 const struct sched_class fair_sched_class;
 
+
+#ifdef CONFIG_CACULE_SCHED
+static inline struct sched_entity *se_of(struct cacule_node *cn)
+{
+	return container_of(cn, struct sched_entity, cacule_node);
+}
+#endif
+
+
+#ifdef CONFIG_CACULE_SCHED
+/*
+ * Enqueue an entity
+ */
+static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *_se)
+{
+	struct cacule_node *se = &(_se->cacule_node);
+
+	se->next = NULL;
+	se->prev = NULL;
+
+	if (cfs_rq->head) {
+		// insert se at head
+		se->next		= cfs_rq->head;
+		cfs_rq->head->prev	= se;
+
+		// lastly reset the head
+		cfs_rq->head		= se;
+	} else {
+		// if empty rq
+		cfs_rq->head = se;
+	}
+}
+
+static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *_se)
+{
+	struct cacule_node *se = &(_se->cacule_node);
+
+	// if only one se in rq
+	if (cfs_rq->head->next == NULL) {
+		cfs_rq->head = NULL;
+	} else if (se == cfs_rq->head) {
+		// if it is the head
+		cfs_rq->head		= cfs_rq->head->next;
+		cfs_rq->head->prev	= NULL;
+	} else {
+		// if in the middle
+		struct cacule_node *prev = se->prev;
+		struct cacule_node *next = se->next;
+
+		prev->next = next;
+		if (next)
+			next->prev = prev;
+	}
+}
+
+struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)
+{
+	if (!cfs_rq->head)
+		return NULL;
+
+	return se_of(cfs_rq->head);
+}
+#else
+
 /**************************************************************
  * CFS operations on generic schedulable entities:
  */
@@ -1224,7 +1361,13 @@ static void update_curr(struct cfs_rq *cfs_rq)
 	if (unlikely(delta_exec <= 0))
 		return;
 
+#ifdef CONFIG_CACULE_SCHED
+	curr->cacule_node.last_run = sched_clock();
 	curr->vruntime += calc_delta_fair(delta_exec, curr);
+	curr->cacule_node.vruntime += calc_delta_fair(delta_exec, curr);
+#else
+	curr->vruntime += calc_delta_fair(delta_exec, curr);
+#endif
 	resched = update_deadline(cfs_rq, curr);
 	update_min_vruntime(cfs_rq);
 
@@ -5239,6 +5382,77 @@ static void
 requeue_delayed_entity(struct sched_entity *se);
 
 static void
+
+#ifdef CONFIG_CACULE_SCHED
+static unsigned int
+calc_interactivity(u64 now, struct cacule_node *se)
+{
+	u64 l_se, vr_se, sleep_se = 1ULL, u64_factor_m, _2m;
+	unsigned int score_se;
+
+	/*
+	 * in case of vruntime==0, logical OR with 1 would
+	 * make sure that the least sig. bit is 1
+	 */
+	l_se		= now - se->cacule_start_time;
+	vr_se		= se->vruntime | 1;
+	u64_factor_m	= interactivity_factor;
+	_2m		= u64_factor_m << 1;
+
+	/* safety check */
+	if (likely(l_se > vr_se))
+		sleep_se = (l_se - vr_se) | 1;
+
+	if (sleep_se >= vr_se)
+		score_se = u64_factor_m / (sleep_se / vr_se);
+	else
+		score_se = _2m - (u64_factor_m / (vr_se / sleep_se));
+
+	return score_se;
+}
+
+static inline int cn_has_idle_policy(struct cacule_node *cn)
+{
+	struct sched_entity *se = se_of(cn);
+
+	if (!entity_is_task(se))
+		return false;
+
+	return task_has_idle_policy(task_of(se));
+}
+
+/*
+ * Does se have lower interactivity score value (i.e. interactive) than curr?
+ * If yes, return 1, otherwise return -1
+ */
+static inline int
+entity_before(u64 now, struct cacule_node *curr, struct cacule_node *se)
+{
+	unsigned int score_curr, score_se;
+	int diff;
+	int is_curr_idle = cn_has_idle_policy(curr);
+	int is_se_idle   = cn_has_idle_policy(se);
+
+	/* if curr is normal but se is idle class, then no */
+	if (!is_curr_idle && is_se_idle)
+		return -1;
+
+	/* if curr is idle class and se is normal, then yes */
+	if (is_curr_idle && !is_se_idle)
+		return 1;
+
+	score_curr	 = calc_interactivity(now, curr);
+	score_se	 = calc_interactivity(now, se);
+
+	diff		= score_se - score_curr;
+
+	if (diff < 0)
+		return 1;
+
+	return -1;
+}
+#endif /* CONFIG_CACULE_SCHED */
+
 enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 {
 	bool curr = cfs_rq->curr == se;
@@ -5461,6 +5675,32 @@ dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 }
 
 static void
+#ifdef CONFIG_CACULE_SCHED
+static struct sched_entity *
+pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
+{
+	struct cacule_node *se = cfs_rq->head;
+	struct cacule_node *next;
+	u64 now = sched_clock();
+
+	if (!se)
+		return curr;
+
+	next = se->next;
+	while (next) {
+		if (entity_before(now, se, next) == 1)
+			se = next;
+
+		next = next->next;
+	}
+
+	if (curr && entity_before(now, se, &curr->cacule_node) == 1)
+		return curr;
+
+	return se_of(se);
+}
+#else
+#endif /* CONFIG_CACULE_SCHED */
 set_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
 {
 	clear_buddies(cfs_rq, se);
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 3c12d9f93..04a961df2 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -85,7 +85,6 @@ SCHED_FEAT(TTWU_QUEUE, true)
 /*
  * When doing wakeups, attempt to limit superfluous scans of the LLC domain.
  */
-SCHED_FEAT(SIS_UTIL, true)
 
 /*
  * Issue a WARN when we do multiple update_rq_clock() calls
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index adfb6e340..a5447cb99 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -695,6 +695,9 @@ struct cfs_rq {
 	 * It is set to NULL otherwise (i.e when none are currently running).
 	 */
 	struct sched_entity	*curr;
+#ifdef CONFIG_CACULE_SCHED
+	struct cacule_node	*head;
+#else
 	struct sched_entity	*next;
 
 	/*
@@ -1147,6 +1150,12 @@ struct rq {
 	struct cfs_rq		cfs;
 	struct rt_rq		rt;
 	struct dl_rq		dl;
+
+#ifdef CONFIG_CACULE_RDB
+	unsigned int		max_IS_score;
+	struct task_struct	*to_migrate_task;
+#endif
+
 #ifdef CONFIG_SCHED_CLASS_EXT
 	struct scx_rq		scx;
 #endif
